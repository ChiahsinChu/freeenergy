\subsection{Multistate Bennett Acceptance Ratio\label{Sec:FEM:MBAR}}
\begin{chapquote}{Ernest Rutherford%, \textit{\url{https://en.wikiquote.org/wiki/Ernest_Rutherford}}
	}
	``An alleged scientific discovery has no merit unless it can be explained to a barmaid.''
\end{chapquote}
\begin{chapquote}{Yihan Shao}
	``So, you can never be a good scientist unless you go to bar regularly.''
\end{chapquote}
The Multistate Bennett Acceptance Ratio (MBAR) method was developed by Shirts and Chodera in 2008.\cite{ShirtsJCP2008} The following derivation quite follows Ref.~\cite{ShirtsarXiv2017}
Imaging you have carried out a series of simulations such as umbrella sampling, or replica exchange molecular dynamics simulations.
Now you have $K$ trajectories in total and each trajectory is characterized by Hamiltonian $H_k$ and inverse temperature $\beta_k$. The trajectories unnecessarily have the same number of conformations. Instead, the number of conformations in trajectory $k$ is $N_k$. Now, you mix all the samples and randomly pick one sample out of them. The probably for this sample to have coordinates $\mathbf{R}$ is 
\begin{equation}
p_m(\mathbf{R})=\frac 1N\sum_{k=1}^{K}N_kp_k(\mathbf{R}),
\end{equation}
in which $N=\sum\limits_{k=1}^{K}N_k$ and the subscript $m$ means mixed ensemble. $p_k(\mathbf{R})$ is the probability of finding this snapshot in trajectory $k$, which satisfies
\begin{equation}
p_k(\mathbf{R})={c_k}^{-1}q_k(\mathbf{R}).
\end{equation}
$c_k$ is the normalization constant. You can see that this mixed ensemble does not follow Boltzmann statistics, even if $q_k$ does. It can be proved that if $p_k$ is normalized, then $p_m$ is also normalized.

The expectation of any operator $\hat{O}$ averaged over this mixed ensemble can be calculated by
\begin{equation}
    \langle O\rangle_m=\int O(\mathbf{R})p_m(\mathbf{R})\diff\mathbf{R}\approx\frac 1N \sum\limits_{n=1}^NO(\mathbf{R}_n).
\end{equation}
Using energy reweighting\cite{TorrieJComputP1977}, we can calculate the expectation of this operator under \emph{any} other Hamiltonian $H_i$ and probability $p_i$, which can be expressed as
\begin{align}
    \langle O\rangle_i=&\int O(\mathbf{R})p_i(\mathbf{R})\diff\mathbf{R}\notag\\
                      =&\int O(\mathbf{R})\frac{p_i(\mathbf{R})}{p_m(\mathbf{R})}p_m(\mathbf{R})\diff\mathbf{R}\notag\\
                      \approx&\frac 1N\sum\limits_{n=1}^N O(\mathbf{R}_n)\frac{p_i(\mathbf{R}_n)}{p_m(\mathbf{R}_n)}\notag\\
                      =&\frac 1N\sum\limits_{n=1}^N O(\mathbf{R}_n){c_i}^{-1}\frac{q_i(\mathbf{R}_n)}{p_m(\mathbf{R}_n)}\notag\\
                      =&\sum\limits_{n=1}^N O(\mathbf{R}_n){c_i}^{-1}\frac{q_i(\mathbf{R}_n)}{\sum_{k=1}^{K}N_kp_k(\mathbf{R}_n)}
    \label{Eq:FEM:MBAR:Oexpect}
\end{align}
Let $O=1$, we find
\begin{equation}
    1=\sum\limits_{n=1}^N {c_i}^{-1}\frac{q_i(\mathbf{R}_n)}{\sum_{k=1}^{K}N_kp_k(\mathbf{R}_n)}.
\end{equation}
Since $c_i$ does not depend on $n$,
\begin{align}
    c_i=&\sum\limits_{n=1}^N \frac{q_i(\mathbf{R}_n)}{\sum_{k=1}^{K}N_kp_k(\mathbf{R}_n)}\notag\\
       =&\sum\limits_{n=1}^N \frac{q_i(\mathbf{R}_n)}{\sum_{k=1}^{K}N_k{c_k}^{-1}q_k(\mathbf{R}_n)}
   \label{Eq:FEM:MBAR:c_i}
\end{align}
In Boltzmann statistics, $q_k(\mathbf{R})=\exp{\left[-\beta_kU_k(\mathbf{R})\right]}$ and $c_k=\int q_k(\mathbf{R})d\mathbf{R}$ is the partition function or the normalization constant. \textit{Note that we have not assumed anything about the statistics of ensemble $k$ and $i$. Besides, $i$ is unnecessarily within \{$k$\}. For instance, if we run replica exchange molecular dynamics simulations at $K$ inverse temperatures $\beta_1,\dots,\beta_K$, $\beta_i$ can be either one of these inverse temperatures or any other inverse temperature between $\beta_1$ and $\beta_K$. But extrapolation to inverse temperatures outside the range of $\left[\beta_K,\beta_1\right]$ is not recommended.}

If $q_k$ and $q_i$ follow Boltzmann statistics, and we define free energy $f_i=-{\beta_i}^{-1}\ln{c_i}$, Eq.~\ref{Eq:FEM:MBAR:c_i} becomes
\begin{equation}
    f_i=-{\beta_i}^{-1}\ln{\sum\limits_{n=1}^N \frac{\exp{\left[-\beta_i U_i(\mathbf{R}_n)\right]}}{\sum_{k=1}^{K}N_k\exp{\left[\beta_kf_k-\beta_kU_k(\mathbf{R}_n)\right]}}},
    \label{Eq:FEM:MBAR:f_i_final}
\end{equation}
which must be solved self-consistently and can be determined up to a constant. We can fix $f_1$ (to 0 usually). 

Again, from Eq.~\ref{Eq:FEM:MBAR:Oexpect}, we can define
\begin{equation}
    W_{in}={c_i}^{-1}\frac{q_i(\mathbf{R}_n)}{\sum_{k=1}^{K}N_k{c_k}^{-1}q_k(\mathbf{R}_n)},
\end{equation}
which is the weight of snapshot $n$ in ensemble $i$ determined by Hamiltonian $H_i$ and temperature $\beta_i$. Specifically, for the Boltzmann statistics,
\begin{align}
    W_{in}=\frac{e^{-\beta_i \left[U_i(\mathbf{R}_n)-f_i\right]}}{\sum_{k=1}^{K}N_k e^{-\beta_k \left[U_k(\mathbf{R}_n)-f_k\right]}}.
\end{align}
Here, $e^{\beta_i f_i}$ in the numerator serves as a normalization factor for $W_{in}$, which is unknown beforehand. Practically, we can define an unnormalized weight function
\begin{equation}
	W_{in}^\prime=\frac{e^{-\beta_i U_i(\mathbf{R}_n)}}{\sum_{k=1}^{K}N_k e^{-\beta_k \left[U_k(\mathbf{R}_n)-f_k\right]}}.
\end{equation} 

As a special case, suppose we have performed a single simulation with an inverse temperature $\beta$ and potential energy function $U_0(\mathbf{R})$. The expectation of an operator $\hat{X}$ under an inverse temperature $\beta$ and potential energy function $U_1(\mathbf{R})$ is
\begin{align}
	\left<X\right>_1=&\frac{\sum_n X(\mathbf{R}_n)W_{1n}^\prime}{\sum_n W_{1n}^\prime}\notag\\
	                =&\frac{\sum_n X(\mathbf{R}_n)\frac{e^{-\beta U_1(\mathbf{R}_n)}}{N e^{-\beta \left[U_0(\mathbf{R}_n)-f_0\right]}}}{\sum_n \frac{e^{-\beta U_1(\mathbf{R}_n)}}{N e^{-\beta \left[U_0(\mathbf{R}_n)-f_0\right]}}}\notag\\
	                =&\frac{\frac1N\sum_n X(\mathbf{R}_n)e^{-\beta \Delta U}}{\frac1N\sum_n e^{-\beta \Delta U}}\notag\\
	                =&\frac{\left<Xe^{-\beta \Delta U}\right>_0}{\left<e^{-\beta \Delta U}\right>_0},
\end{align}
which returns to Eq.~\ref{Eq:ES:US:reweighting}. Similarly, Eq.~\ref{Eq:FEM:MBAR:f_i_final} becomes
\begin{equation}
    \exp{(-\beta f_1)}=\sum\limits_{n=1}^N \frac{\exp{\left(-\beta U_1(\mathbf{R}_n)\right)}}{N\exp{\left(\beta f_0-\beta U_0(\mathbf{R}_n)\right)}}.
\end{equation}
By moving the term $\exp{(-\beta f_0)}$ to left hand side of this equation, we have
\begin{equation}
    \Delta f=-\beta^{-1}\ln{\sum\limits_{n=1}^N\frac{1}{N}\exp{(-\beta \Delta U)}}=-\beta^{-1}\ln{\left<\exp{(-\beta \Delta U)}\right>_0},
\end{equation}
which is Eq.~\ref{Eq:FEM:TP:deltaA5}.

Equation~\ref{Eq:FEM:MBAR:f_i_final} can also be solved using the Newton-Raphson algorithm by defining the residual $\mathbf{r}$ with elements
\begin{equation}
    r_i=1-\sum_{n=1}^{N}\frac{\exp{\left[\beta_i f_i-\beta_i U_i(\mathbf{R}_n)\right]}}{\sum_{k=1}^{K}N_k\exp{\left[\beta_kf_k-\beta_kU_k(\mathbf{R}_n)\right]}}.
\end{equation}
With the current value of $\mathbf{f^\nu}=[f_1^\nu,f_2^\nu,\dots,f_K^\nu]^T$ and $\mathbf{r}(\mathbf{f}^\nu)\neq \mathbf{0}$, we look for $\Delta \mathbf{f}^\nu$ so that $\mathbf{r}(\mathbf{f}^\nu+\Delta \mathbf{f}^\nu)=\mathbf{0}$. Truncating after the first-order term of the Taylor expansion, $\mathbf{r}(\mathbf{f}^\nu+\Delta \mathbf{f}^\nu)$ can be approximately expressed as
\begin{equation}
    \mathbf{r}(\mathbf{f}^\nu+\Delta \mathbf{f}^\nu)\approx \mathbf{r}(\mathbf{f}^\nu) + \mathbf{J}^\nu\Delta \mathbf{f}^\nu,
\end{equation}
where $\mathbf{J}^\nu$ is the $K\times K$ Jacobian:
\begin{equation}
    \mathbf{J}^\nu=\begin{bmatrix}
        \frac{\partial r_1(\mathbf{f}^\nu)}{\partial f_1^\nu}&\frac{\partial r_1(\mathbf{f}^\nu)}{\partial f_2^\nu}&\dots& \frac{\partial r_1(\mathbf{f}^\nu)}{\partial f_K^\nu}\\
        \frac{\partial r_2(\mathbf{f}^\nu)}{\partial f_1^\nu}&\frac{\partial r_2(\mathbf{f}^\nu)}{\partial f_2^\nu}&\dots& \frac{\partial r_2(\mathbf{f}^\nu)}{\partial f_K^\nu}\\
        \vdots&\vdots&\ddots&\vdots\\
        \frac{\partial r_K(\mathbf{f}^\nu)}{\partial f_1^\nu}&\frac{\partial r_K(\mathbf{f}^\nu)}{\partial f_2^\nu}&\dots& \frac{\partial r_K(\mathbf{f}^\nu)}{\partial f_K^\nu}
    \end{bmatrix}
\end{equation}
with
\begin{align}
	\mathrm{J}^\nu_{ji}=&\frac{\partial r_i(\mathbf{f}^\nu)}{\partial f_j^\nu}\notag\\
	                   =&\sum_{n=1}^N\frac{N_j\beta_j\exp{\left[\beta_i f_i-\beta_i U_i(\mathbf{R}_n)\right]}\exp{\left[\beta_j f_j-\beta_j U_j(\mathbf{R}_n)\right]}}{\left\{\sum_{k=1}^K N_k\exp{\left[\beta_kf_k-\beta_kU_k(\mathbf{R}_n)\right]}\right\}^2}\notag\\
	                    &-\sum_{n=1}^N\frac{\beta_i\exp{\left[\beta_i f_i-\beta_i U_i(\mathbf{R}_n)\right]}}{\sum_{k=1}^K N_k\exp{\left[\beta_kf_k-\beta_kU_k(\mathbf{R}_n)\right]}}\delta_{ij}.
\end{align}
Since $\mathbf{r}(\mathbf{f}^\nu+\Delta \mathbf{f}^\nu)=\mathbf{0}$, we find
\begin{equation}
    \Delta \mathbf{f}^\nu\approx -\left[\mathbf{J}^\nu\right]^{-1}\mathbf{r}(\mathbf{f}^\nu).
\end{equation}
and
\begin{equation}
    \mathbf{f}^{\nu+1}=\mathbf{f}^\nu+\Delta \mathbf{f}^\nu
\end{equation}
This iteration continues until this convergence has been reached by checking if $|\Delta \mathbf{f}|<\boldsymbol{\epsilon}$.


Ding et al\cite{DingJCTC2019} showed that solving Eq.~\ref{Eq:FEM:MBAR:f_i_final} is equivalent to minimizing
\begin{equation}
    f(b_1,b_2,\dots,b_K)=\frac{1}{N}\sum_{k=1}^{K}\sum_{j=1}^{N_k}\ln{\left\{\sum_{l=1}^{K}\exp{\left[-u_l(\mathbf{R}_j^{k})-b_l\right]}\right\}}+\sum_{k=1}^{K}\frac{N_k}{N}b_k
\end{equation}
with respect to $b_k$.
